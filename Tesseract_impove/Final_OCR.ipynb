{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_OCR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrn8pt7kXwpVMowGpH4dIs"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H9xY7vbVWkz",
        "outputId": "f7f75395-436a-416f-8291-40113261c2d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2quU83-MpNwQ",
        "outputId": "76aade10-4d80-4dfe-c1f9-55fb1c10a02e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install -y python-enchant"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko python-gobject python-gtk2\n",
            "  python-wxgtk3.0\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "  python-enchant\n",
            "0 upgraded, 11 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 1,354 kB of archives.\n",
            "After this operation, 5,584 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.1 [309 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.1 [87.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-enchant all 2.0.0-1 [44.1 kB]\n",
            "Fetched 1,354 kB in 1s (999 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 144611 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../01-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../02-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../03-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../04-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../05-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../06-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../07-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../08-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../09-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Selecting previously unselected package python-enchant.\n",
            "Preparing to unpack .../10-python-enchant_2.0.0-1_all.deb ...\n",
            "Unpacking python-enchant (2.0.0-1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up python-enchant (2.0.0-1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlgkhGUTpKRj",
        "outputId": "12481e6e-ed41-47c0-eb41-5e4c545aeaa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 제일 먼저 tesseract 설치한다\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract \n",
        "!pip install opencv-python\n",
        "!pip install imutils\n",
        "!pip install numpy\n",
        "!pip install pillow\n",
        "!pip install nltk\n",
        "!pip install tensorflow\n",
        "!pip install PyEnchant\n",
        "!pip install langdetect"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 2s (2,920 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 145051 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading https://files.pythonhosted.org/packages/17/4b/4dbd55388225bb6cd243d21f70e77cb3ce061e241257485936324b8e920f/pytesseract-0.3.6.tar.gz\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from pytesseract) (7.0.0)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.6-py2.py3-none-any.whl size=13629 sha256=2f950ae36289644ccd4248c7ad7e4f0f9c00bc216e63a638aebc09b9cf4f7adf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/71/72/b98430261d849ae631e283dfc7ccb456a3fb2ed2205714b63f\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.6\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.5)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.6/dist-packages (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Collecting PyEnchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/55/810c871d9a556685553ab1ace4a6c580460ca476736829fffe8cfef32a66/pyenchant-3.1.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.3MB/s \n",
            "\u001b[?25hInstalling collected packages: PyEnchant\n",
            "Successfully installed PyEnchant-3.1.1\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=43cf6c045f446627ac37359f9449c26929db151d9d422133d62ac473d7448d6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD15_wnzpW8M",
        "outputId": "c55f2af1-6728-4e00-f2a4-a6d3d3d2a755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f6cq8tnpLSP"
      },
      "source": [
        "# 필요한 모듈 \n",
        "import os\n",
        "import cv2\n",
        "import pytesseract\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import re\n",
        "import enchant\n",
        "from google.colab.patches import cv2_imshow\n",
        "import imutils\n",
        "from langdetect import detect_langs\n",
        "%matplotlib inline"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxPzETSj-6VA"
      },
      "source": [
        "### **OCR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OuMrUKipf7E"
      },
      "source": [
        "class OCR1():\n",
        "  def __init__(self, img):\n",
        "    self.config = r'--oem 3 --psm 3 -c preserve_interword_spaces=1'\n",
        "    if img.shape[1] < 500:\n",
        "      self.image = cv2.cvtColor(imutils.resize(img, width=500), cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "      self.image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    cv2_imshow(self.image)\n",
        "    self.result = ''\n",
        "    self.filter_math = '[^A-Za-z0-9+-/*÷=×±∓∘∙∩∪≅∀√%∄∃θπσ≠<>≤≥≡∼≈≢∝≪≫∈∋∉⊂⊃⊆⊇⋈∑∫∏∞x()]'\n",
        "    self.language = enchant.Dict(\"en_US\")\n",
        "    self.english_score = []\n",
        "    self.hindi_score = []\n",
        "\n",
        "  def image_pytesseract(self):\n",
        "    if not self.text_filtering():\n",
        "      print(\"\\ntwice text_filtering\")\n",
        "     \n",
        "      blur =  cv2.GaussianBlur(self.image, (5,5), 0)\n",
        "      self.image = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "      cv2_imshow(self.image)\n",
        "      self.text_filtering()\n",
        "\n",
        "  def text_filtering(self):\n",
        "    self.result = WordPunctTokenizer().tokenize(pytesseract.image_to_string(self.image, lang='eng', config=self.config))\n",
        "    #print(\"Original Nltk tokenizer Before : \",self.result)\n",
        "    self.result = [re.sub(self.filter_math, '', i).lower() for i in self.result if re.sub(self.filter_math, '', i) != '']\n",
        "    #print(\"After Nltk tokenizer, filtering : \", self.result)\n",
        "    \n",
        "    if len(self.result) <= 3: \n",
        "      print(\"False\\n\")\n",
        "      return False\n",
        "    else:\n",
        "      Ncount = 0\n",
        "      Ccount = 0\n",
        "      Tcount = 0\n",
        "      Fcount = 0\n",
        "      for i in self.result:\n",
        "        if i.isdigit(): #숫자\n",
        "          print(\"this is num: \",i)\n",
        "          Ncount += 1\n",
        "        else: #숫자 외 -> 연산자 or 영어 단어\n",
        "          if i in '+-/*÷=×±∓∘∙∩∪≅∀√%∄∃θπσ≠<>≤≥≡∼≈≢∝≪≫∈∋∉⊂⊃⊆⊇⋈∑∫∏∞x,.()':\n",
        "            print(\"this is math operations : \", i, end=\"\\n\")\n",
        "            Ccount += 1\n",
        "          else:\n",
        "            print(\"this is character : \", i,self.language.check(i),end=\"\\n\")\n",
        "            if self.language.check(i): Tcount += 1\n",
        "            else: Fcount += 1\n",
        "      print(\"\\ncount: \", Ncount)\n",
        "      print(\"Ccount: \",Ccount)\n",
        "      print(\"Tcount: \",Tcount)\n",
        "      print(\"Fcount: \",Fcount)\n",
        "      \n",
        "      if (Ccount + Ncount + Tcount < 6) or ((Ccount + Ncount < Tcount + Fcount) and (Tcount <= Fcount)): # false일 경우\n",
        "        print(\"False\")\n",
        "        return False\n",
        "      else:\n",
        "        print(\"True\")\n",
        "        return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlbp5CSP_0-w"
      },
      "source": [
        "# gray + gaussian + threshold\n",
        "\n",
        "class OCR2():\n",
        "  def __init__(self, img):\n",
        "    self.config = r'--oem 3 --psm 3 -c preserve_interword_spaces=1'\n",
        "    if img.shape[1] < 500:\n",
        "      self.img = cv2.cvtColor(imutils.resize(img, width=500), cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "      self.img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    self.blur =  cv2.GaussianBlur(self.img, (5,5), 0)\n",
        "    self.image = cv2.threshold(self.blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "    cv2_imshow(self.image)\n",
        "    self.result = ''\n",
        "    self.filter_math = '[^A-Za-z0-9+-/*÷=×±∓∘∙∩∪≅∀√%∄∃θπσ≠<>≤≥≡∼≈≢∝≪≫∈∋∉⊂⊃⊆⊇⋈∑∫∏∞x]'\n",
        "    self.language = enchant.Dict(\"en_US\")\n",
        "\n",
        "  def image_pytesseract(self):\n",
        "    if not self.text_filtering():\n",
        "      print(\"\\ntwice text_filtering\")\n",
        "      self.image = self.img\n",
        "      cv2_imshow(self.image)\n",
        "      self.text_filtering()\n",
        "\n",
        "  def text_filtering(self):\n",
        "    self.result = WordPunctTokenizer().tokenize(pytesseract.image_to_string(self.image, lang='eng', config=self.config))\n",
        "    print(\"Original Nltk tokenizer Before : \",self.result)\n",
        "    self.result = [re.sub(self.filter_math, '', i).lower() for i in self.result if re.sub(self.filter_math, '', i) != '']\n",
        "    print(\"After Nltk tokenizer, filtering : \", self.result)\n",
        "    \n",
        "    if len(self.result) <= 3: \n",
        "      print(\"False\\n\")\n",
        "      return False\n",
        "    else:\n",
        "      Ncount = 0\n",
        "      Ccount = 0\n",
        "      Tcount = 0\n",
        "      Fcount = 0\n",
        "      for i in self.result:\n",
        "        if i.isdigit(): #숫자\n",
        "          print(\"this is num: \",i)\n",
        "          Ncount += 1\n",
        "        else: #숫자 외 -> 연산자 or 영어 단어\n",
        "          if i in '+-/*÷=×±∓∘∙∩∪≅∀√%∄∃θπσ≠<>≤≥≡∼≈≢∝≪≫∈∋∉⊂⊃⊆⊇⋈∑∫∏∞x,.':\n",
        "            print(\"this is math operations : \", i, end=\"\\n\")\n",
        "            Ccount += 1\n",
        "          else:\n",
        "            print(\"this is character : \", i,self.language.check(i),end=\"\\n\")\n",
        "            if self.language.check(i): Tcount += 1\n",
        "            else: Fcount += 1\n",
        "      print(\"\\ncount: \", Ncount)\n",
        "      print(\"Ccount: \",Ccount)\n",
        "      print(\"Tcount: \",Tcount)\n",
        "      print(\"Fcount: \",Fcount)\n",
        "      \n",
        "      if (Ccount + Ncount + Tcount < 6) or ((Ccount + Ncount < Tcount + Fcount) and (Tcount <= Fcount)): # false일 경우\n",
        "        print(\"False\")\n",
        "        return False\n",
        "      else:\n",
        "        print(\"True\")\n",
        "        return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ASqbWPX-4Qo"
      },
      "source": [
        "# 조건문 추가하기\n",
        "\n",
        "class OCR3():\n",
        "  def __init__(self, img):\n",
        "    self.config = r'--oem 3 --psm 3 -c preserve_interword_spaces=1'\n",
        "    if img.shape[1] < 500:\n",
        "      self.image = cv2.cvtColor(imutils.resize(img, width=400), cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "      self.image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    cv2_imshow(self.image)\n",
        "    self.result = ''\n",
        "    self.filter_math = '[^A-Za-z0-9+-/*÷=×±∓∘∙∩∪≅∀√%∄∃θπσ≠<>≤≥≡∼≈≢∝≪≫∈∋∉⊂⊃⊆⊇⋈∑∫∏∞x]'\n",
        "    self.language = enchant.Dict(\"en_US\")\n",
        "\n",
        "  def image_pytesseract(self):\n",
        "    if not self.text_filtering():\n",
        "      print(\"\\ntwice text_filtering\")\n",
        "     \n",
        "      blur =  cv2.GaussianBlur(self.image, (5,5), 0)\n",
        "      self.image = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "      cv2_imshow(self.image)\n",
        "      self.text_filtering()\n",
        "\n",
        "  def text_filtering(self):\n",
        "    self.result = WordPunctTokenizer().tokenize(pytesseract.image_to_string(self.image, lang='eng', config=self.config))\n",
        "    print(\"Original Nltk tokenizer Before : \",self.result)\n",
        "    self.result = [re.sub(self.filter_math, '', i).lower() for i in self.result if re.sub(self.filter_math, '', i) != '']\n",
        "    print(\"After Nltk tokenizer, filtering : \", self.result)\n",
        "    \n",
        "    if len(self.result) <= 3: \n",
        "      print(\"False\\n\")\n",
        "      return False\n",
        "    else:\n",
        "      Ncount = 0\n",
        "      Ccount = 0\n",
        "      Tcount = 0\n",
        "      Fcount = 0\n",
        "      for i in self.result:\n",
        "        if i.isdigit(): #숫자\n",
        "          print(\"this is num: \",i)\n",
        "          Ncount += 1\n",
        "        else: #숫자 외 -> 연산자 or 영어 단어\n",
        "          if i in '+-/*÷=×±∓∘∙∩∪≅∀√%∄∃θπσ≠<>≤≥≡∼≈≢∝≪≫∈∋∉⊂⊃⊆⊇⋈∑∫∏∞x,.()':\n",
        "            print(\"this is math operations : \", i, end=\"\\n\")\n",
        "            Ccount += 1\n",
        "          else:\n",
        "            temp1, temp2 = [idx for idx in range(len(i)) if i[idx].isdigit()], [idx for idx in range(len(i)) if i[idx] in '+-/*÷=×±∓∘∙∩∪≅∀√%∄∃θπσ≠<>≤≥≡∼≈≢∝≪≫∈∋∉⊂⊃⊆⊇⋈∑∫∏∞x,.()']\n",
        "            print(\"this is character : \", i,self.language.check(i),end=\"\\n\")\n",
        "            if temp1 or temp2:\n",
        "              Ncount += len(temp1)\n",
        "              Ccount += len(temp2)\n",
        "            else:\n",
        "              if self.language.check(i): Tcount += 1\n",
        "              else: Fcount += 1\n",
        "      print(\"\\ncount: \", Ncount)\n",
        "      print(\"Ccount: \",Ccount)\n",
        "      print(\"Tcount: \",Tcount)\n",
        "      print(\"Fcount: \",Fcount)\n",
        "      \n",
        "      if ((Ccount + Ncount < Tcount + Fcount) and (Tcount <= Fcount)) or ((Tcount + Fcount >= 15) and (abs(Tcount - Fcount) <= 3)): # false일 경우 | (Ccount + Ncount + Tcount < 6) or \n",
        "        print(\"False\")\n",
        "        return False\n",
        "      else:\n",
        "        print(\"True\")\n",
        "        return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1D0Wm0t_-K_"
      },
      "source": [
        "# 조건문 추가하기\n",
        "\n",
        "class OCR4():\n",
        "  def __init__(self, img):\n",
        "    self.config = r'--oem 3 --psm 3 -l eng+hin -c preserve_interword_spaces=1'\n",
        "    if img.shape[1] < 500:\n",
        "      self.image = cv2.cvtColor(imutils.resize(img, width=400), cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "      self.image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    cv2_imshow(self.image)\n",
        "    self.result = ''\n",
        "    self.filter_math = '[^A-Za-z0-9+-/*÷=×±∓∘∙∩∪≅∀√%∄∃θπσ≠<>≤≥≡∼≈≢∝≪≫∈∋∉⊂⊃⊆⊇⋈∑∫∏∞x]'\n",
        "    self.language = enchant.Dict(\"en_US\")\n",
        "\n",
        "  def image_pytesseract(self):\n",
        "    if not self.text_filtering():\n",
        "      print(\"\\ntwice text_filtering\")\n",
        "     \n",
        "      self.image =  cv2.GaussianBlur(self.image, (5,5), 0)\n",
        "      #self.image = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "      cv2_imshow(self.image)\n",
        "      self.text_filtering()\n",
        "\n",
        "  def text_filtering(self):\n",
        "    text = pytesseract.image_to_string(self.image, config=self.config)#lang='eng+hin',\n",
        "    try:\n",
        "      print(\"detect lang: \", detect_langs(text))\n",
        "      \n",
        "    except: return\n",
        "    self.result = WordPunctTokenizer().tokenize(text)\n",
        "    print(\"Original Nltk tokenizer Before : \",self.result)\n",
        "    self.result = [re.sub(self.filter_math, '', i).lower() for i in self.result if re.sub(self.filter_math, '', i) != '']\n",
        "    print(\"After Nltk tokenizer, filtering : \", self.result)\n",
        "    \n",
        "    if len(self.result) <= 3: \n",
        "      print(\"False\\n\")\n",
        "      return False\n",
        "    else:\n",
        "      Ncount = 0\n",
        "      Ccount = 0\n",
        "      Tcount = 0\n",
        "      Fcount = 0\n",
        "      for i in self.result:\n",
        "        if i.isdigit(): #숫자\n",
        "          print(\"this is num: \",i)\n",
        "          Ncount += 1\n",
        "        else: #숫자 외 -> 연산자 or 영어 단어\n",
        "          if i in '+-/*÷=×±∓∘∙∩∪≅∀√%∄∃θπσ≠<>≤≥≡∼≈≢∝≪≫∈∋∉⊂⊃⊆⊇⋈∑∫∏∞x,.()':\n",
        "            print(\"this is math operations : \", i, end=\"\\n\")\n",
        "            Ccount += 1\n",
        "          else:\n",
        "            temp1, temp2 = [idx for idx in range(len(i)) if i[idx].isdigit()], [idx for idx in range(len(i)) if i[idx] in '+-/*÷=×±∓∘∙∩∪≅∀√%∄∃θπσ≠<>≤≥≡∼≈≢∝≪≫∈∋∉⊂⊃⊆⊇⋈∑∫∏∞x,.()']\n",
        "            print(\"this is character : \", i,self.language.check(i),end=\"\\n\")\n",
        "            if temp1 or temp2:\n",
        "              Ncount += len(temp1)\n",
        "              Ccount += len(temp2)\n",
        "            else:\n",
        "              if self.language.check(i): Tcount += 1\n",
        "              else: Fcount += 1\n",
        "      print(\"\\ncount: \", Ncount)\n",
        "      print(\"Ccount: \",Ccount)\n",
        "      print(\"Tcount: \",Tcount)\n",
        "      print(\"Fcount: \",Fcount)\n",
        "      \n",
        "      if ((Ccount + Ncount < Tcount + Fcount) and (Tcount <= Fcount)) or ((Tcount + Fcount >= 10) and (abs(Tcount - Fcount) < 3)): # false일 경우 | (Ccount + Ncount + Tcount < 6) or \n",
        "        print(\"False\")\n",
        "        return False\n",
        "      else:\n",
        "        print(\"True\")\n",
        "        return True\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_HW0uw0-g3f"
      },
      "source": [
        "### **real data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNSmTpn9x287"
      },
      "source": [
        "name = ['general_Capture','hindi','general_photo','image','handwritten']\n",
        "path_dir = '/content/drive/My Drive/OCR/TEST_OCR1/real_Test/'\n",
        "file_list = os.listdir(path_dir)\n",
        "file_list = [(i, [path_dir + i + '/' + j for j in os.listdir(path_dir+i)]) for i in file_list]\n",
        "file_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQMtWj1mMfOq"
      },
      "source": [
        "OCR3가 가장 정확함\n",
        "\n",
        "아래는 OCR3 테스트 결과 의미\n",
        "\n",
        "\n",
        "텍스트이미지 부분\n",
        "<br>\n",
        "t24 -> 화질?<br>\n",
        "t22 -> ..?? 화질?<br>\n",
        "t21 -> ..??<br>\n",
        "t2 -> 필기체<br>\n",
        "t18 -> 화질<br>\n",
        "t14 -> 아예 못읽어 드림(글씨체 문제인가?)<br>\n",
        "t13 -> 글씨체..?<br>\n",
        "t12 -> 글씨체<br>\n",
        "t31 -> 어두움 + 글씨체<br>\n",
        "t19 -> 어두움 + 안보임<br>\n",
        "t8 -> 글씨 흐림 <br>\n",
        "t6 -> 글씨체 + 흐림<br>\n",
        "t40 -> 화질, 글씨체, 흐림<br>\n",
        "t43 -> 화질, 흐림<br>\n",
        "t39 -> 화질, 글씨 안보임<br>\n",
        "t37 -> 글씨체<br>\n",
        "t34 -> 글씨체 + 멀리서 찍음<br>\n",
        "t35 -> 너무 어두움<br>\n",
        "t30 -> 글씨체 + 흐림<br>\n",
        "t27 -> 글씨체<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keIfyE9oBWm7"
      },
      "source": [
        "# OCR4 TEST\n",
        "\n",
        "for i in file_list:\n",
        "  print(i[0])\n",
        "  for j in i[1]:\n",
        "    print(j)\n",
        "    image = cv2.imread(j)\n",
        "\n",
        "    MyOCR = OCR4(img=image) # i == image\n",
        "    MyOCR.image_pytesseract()\n",
        "  print(\"\\n\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}